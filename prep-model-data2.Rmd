---
title: "Model Data Preparation (newer)"
output: html_document
date: "`r Sys.Date()`"
---

---
title: "Model Data Preparation"
output: html_document
date: "`r Sys.Date()`"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

The output of this script is a spatial dataframe of points that is ready to be fed to the models. This will contain two "types" of points, unobserved points and observed ones. The observed points are the centroids of the park, for each time a particular site was visited, while the unobserved points lie on a 4km grid across South Carolina and will be used for prediction.

The strategy for preparing everything is first to obtain the grid using NLCD 2019 land cover classification. The land cover is one of our covariates of interest for modeling, but they also provide a grid so we can start there. Then we will build up this grid with other meteorological variables in monthly or yearly increments, depending on the variable. Separately, we then extract all the variables from the observed points, and then both spatial dataframes are combined.

The current covariates are (TODO)
* Land cover (numeric label)
* Tree canopy (precent tree cover 0-100)
* Average monthly temperature (Celsius)
* Cumulative monthly precipitation (mm)
* Average monthly relative humidity, calculated using mean temperature and dew point (0-100)
* Annual minimum temperature, defined as the minimum daily temperature, averaged over January

Load required packages
```{r, message=FALSE}
library(tidyverse)
library(sf)
library(raster, exclude=c("select"))
library(furrr)
library(lubridate)
library(nngeo)
```

Load the data
```{r}
parks <- read_csv("data-proc/parks-data-20-21.csv") # read data as created in tidy-parks-data.R
```

## Park data aggregation

Before extracting covariates, do some preparations of the park data itself, aggregating and dropping data where
needed. The following steps are taken preparing the data for model fitting:

- A `month` variable is added with an integer representing the Month of the visit, as well as a `year` variable
- The variables `sampling_method`, `species` and `sex` will not be used in the analysis, so we can marginalize these out to obtain counts for the factors we are interested in
- For each visitation event (i.e. `site` and `date` pair), implicit zero counts are converted to explicit zero counts for each group. Note this should also remove the issue that "zero-count" visits were entered inconsistently (some had different genus/species combinations entered but with 0, some were `NA`, etc.)
- We the `nest` the data pertaining to counts for each visit. This allows having a _visit_, the unit of analysis of importance for the covariates, to be a single row. Everything is then unnested at the end, since we are interested in modelling counts.

```{r }
parks_agg <- parks |> 
    select(date, site, genus:count, sampling_method, lat, long) |> 
    mutate(month=month(date), year=year(date)) |> 
    group_by(date, site, genus, species, life_stage, month, year, lat, long) |> # marginalize over factors not considered 
    summarise(count=sum(count)) |> 
    ungroup() |> 
    complete(nesting(date, site, month, year, lat, long), nesting(genus, species, life_stage), fill=list(count=0)) |> 
    nest(counts=c(genus, species, life_stage, count))
```

Finally, we use the `lat` and `long` variables to project the data to a given CRS using the following function. `ref_raster` is a folder name containing raster files we will be using later. For now we use it only as a reference CRS, since it will very slow to convert these raster files to a different CRS.

```{r}
project_parks <- function(parks, ref_raster="min-temp", pattern="bil$") {
    files <- list.files(paste0("geo-files/", ref_raster), pattern=pattern, full.names=TRUE)
    
    parks |>
        st_as_sf(crs="+proj=longlat +datum=WGS84", coords=c("long", "lat")) |>
        st_transform(crs(raster(files[1])))
}

parks_proj <- project_parks(parks_agg, "min-temp") # now can get reference CRS from here
```

## Create a spatial grid using the NLCD 2019 data

These come as raster files with a 30m x 30m resolution. However, since PRISM only reports their variables at a 4km resolution, and since we will not be able to make monthly predictions at such a fine scale, the grid is aggregated to roughly 4km using the most dominant class within a 4km tile.

The rasters are then converted to points form, creating a dataframe of points along a grid. The dataframe is then filtered to only contain points within the South Carolina border.

```{r}
get_landcover_grid <- function(bounds, saved_file=NULL) {
    if (!is.null(saved_file))
        return(st_read(saved_file))
    
    lcov <- raster("geo-files/land-cover/NLCD_2019_Land_Cover_L48_20210604_yDj0p7IM7PAX2WUUipna.tiff")
    lcov_4k <- aggregate(lcov, 133*4, fun=modal)
    
    lcov_sp <- rasterToPoints(lcov_4k, spatial=TRUE) |> 
        st_as_sf() |> 
        rename(land_cover=NLCD_2019_Land_Cover_L48_20210604_yDj0p7IM7PAX2WUUipna) |> 
        st_transform(crs(bounds))
    
    u <- st_within(lcov_sp, bounds)
    return(lcov_sp[map_lgl(u, ~length(.x) > 0),])
}

get_canopy_grid <- function(bounds, saved_file=NULL) {
    if (!is.null(saved_file))
        return(st_read(saved_file))
    
    canopy <- raster("geo-files/land-cover/NLCD_2016_Tree_Canopy_L48_20190831_yDj0p7IM7PAX2WUUipna.tiff")
    canopy_4k <- aggregate(canopy, 133*4, fun=mean)
    
    canopy_sp <- rasterToPoints(canopy_4k, spatial=TRUE) |> 
        st_as_sf() |> 
        rename(tree_canopy=NLCD_2016_Tree_Canopy_L48_20190831_yDj0p7IM7PAX2WUUipna) |> 
        st_transform(crs(bounds))
    
    u <- st_within(canopy_sp, bounds)
    return(canopy_sp[map_lgl(u, ~length(.x) > 0),])
}
```

```{r}
sc_state <- st_read("geo-files/south-carolina-county-boundaries.shp") |> 
    st_transform(crs(parks_proj)) |> 
    st_union() |> 
    nngeo::st_remove_holes()
```

The grid can be loaded like this:

```
lcov_grid <- get_landcover_grid(sc_state) # grid will be proj to state (which should be proj to parks_proj)
st_write(lcov_grid, "geo-files/landcover-grid.shp", delete_dsn=TRUE)
```
However, the aggregation can take a minute or two, so instead we load the grids made previously, and combine them.

```{r}
# lcov_grid <- get_landcover_grid(NULL, saved_file="geo-files/landcover-grid.shp")
# canopy_grid <- get_canopy_grid(NULL, saved_file="geo-files/canopy-grid.shp") |> 
#     rename(tree_canopy=tr_cnpy)
lcov_grid <- get_landcover_grid(sc_state)
canopy_grid <- get_canopy_grid(sc_state)

nlcd_grid <- st_join(lcov_grid, canopy_grid)
```

Now we make sure both covariates look reasonable

```{r}
ggplot(nlcd_grid) +
    geom_sf(aes(col=as.factor(land_cover)), size=1)

ggplot(nlcd_grid) +
    geom_sf(aes(col=tree_canopy), size=1) +
    scale_color_distiller(palette="Greens", direction=1)
```

Finally, we add in some "empty" columns of `site` and `counts` data for consistency with the observed park points.

```{r}
blank_counts <- parks_agg$counts[[1]] |> mutate(count=NA) # dummy df NA counts (NA indicates to INLA to predict at these locations)

nlcd_grid <- nlcd_grid |> 
    mutate(site=NA, counts=list(blank_counts))
```

## Monthly average of meteriological variables

Using data from [PRISM](https://prism.oregonstate.edu/recent/). To do this, we begin by defining several functions that will be used for all the PRISM data extractions.

```{r}
get_raster_stack <- function(var, dates) {
    file_dates <- str_replace_all(dates, "-", "") |> str_sub(1, 6) # only need year+month for monthly data
    pattern <- str_c(".+_(", str_c(file_dates, collapse="|"), ").+\\.bil$")
    files <- list.files(paste0("geo-files/", var), pattern=pattern, full.names=TRUE)
    as.list(stack(files))
}

load_prism_var <- function(var, dates, points) {
    stack <- get_raster_stack(var, dates)
    varname <- str_replace_all(var, "-", "_")
    
    furrr::future_map2_dfr(
        stack, dates, 
        extract_raster_points, points, 
        .options=furrr_options(seed=NULL)
    ) |> 
        rename({{varname}} := rval)
}

extract_raster_points <- function(r, date, points) {
    rvals <- raster::extract(r, points)

    points |> 
        st_as_sf() |> 
        mutate(rval=rvals, date=date)
}
```

First, set up a parallel session to take advantage of `furrr` and split up the expensive extraction work.

```{r}
plan(multisession, workers=4) # set up parallel session
```

```{r}
dates_grid <- c(str_c("2020", 3:12, "01", sep="-"), str_c("2021", 3:12, "01", sep="-")) |> 
    as.Date()

mintemp_grid <- load_prism_var("min-temp", dates_grid, nlcd_grid) # No pipe cause each line expensive!
maxtemp_grid <- load_prism_var("max-temp", dates_grid, nlcd_grid)
precip_grid <- load_prism_var("precipitation", dates_grid, nlcd_grid)
dew_grid <- load_prism_var("dew-point", dates_grid, nlcd_grid)
```

As an example, we can view the precipitation on April 1st, 2020:

```{r}
ggplot(filter(precip_grid, date == "2020-04-01")) +
    geom_sf(aes(col=precipitation), size=1)
```

Now combine the four variables and convert the dew point/temperature to mean relative humidity. We also add the Month and Year so this can be matched with the observed data later.

```{r}
# From Alduchov & Eskridge (2002)
relative_humidity <- function(mean_temp, dew, beta=17.625, lambda=243.04) {
    num <- exp(beta * dew / (lambda + dew))
    denom <- exp(beta * mean_temp / (lambda + mean_temp))
    100 * num / denom
}

covar_grid <- mintemp_grid |>
    mutate(
        month=month(date),
        year=year(date),
        max_temp=maxtemp_grid$max_temp,
        precipitation=precip_grid$precipitation, 
        mean_rh=relative_humidity((max_temp + min_temp) / 2, dew_grid$dew_point)
    )
```

## Annual minima on grid

Now add annual temperature minima for 2020 and 2021, **defined as the minimum daily temperature at a location, averaged over the month of January**, again using data from PRISM. These new values are then added to `covar_grid` as a new column.

```{r}
dates_min <- as.Date(str_c(c("2020", "2021"), "-01-01"))

tmin <- load_prism_var("min-temp", dates_min, nlcd_grid) |> 
    mutate(year=year(date))

covar_grid <- st_join(covar_grid, select(tmin, year, jan_min_temp=min_temp)) |> 
    filter(year.x == year.y) |> 
    rename(year=year.x) |> 
    select(-year.y)
```

Let's make the minimum temperature looks reasonable:

```{r}
ggplot(covar_grid) +
    geom_sf(aes(col=jan_min_temp), size=0.9) +
    scale_color_viridis_c() +
    labs(col="Mean January minimum (C)")
```

and save the grid of covariates for later.

```{r}
st_write(covar_grid, "geo-files/covar-grid.shp", delete_dsn=TRUE)
```


## Extract covariates from park locations

Now we can repeat the extraction from the centroid of each park. First, to obtain the NLCD variables, we assign a park the value of the closest point on the previously made `covar_grid`.

```{r}
grid_sub <- select(covar_grid, land_cover, tree_canopy) # to avoid ugly st_join behaviour
parks_proj <- st_join(parks_proj, grid_sub, st_nearest_feature)
```

And now the meteorological covariates, again using the `load_prism_var` function, but this time with the points of each each site.

```{r}
park_locations <- distinct(parks_proj, site, geometry)

ann_mintemp_park <- load_prism_var("min-temp", dates_min, park_locations)
mintemp_park <- load_prism_var("min-temp", dates_grid, park_locations)
maxtemp_park <- load_prism_var("max-temp", dates_grid, park_locations)
precip_park <- load_prism_var("precipitation", dates_grid, park_locations)
dew_park <- load_prism_var("dew-point", dates_grid, park_locations)
```

Combine the results, and join with the parks data so that tick `count` and the covariates appear together.

```{r}
covars_park <- mintemp_park |> 
    mutate(
        max_temp=maxtemp_park$max_temp,
        precipitation=precip_park$precipitation, 
        mean_rh=relative_humidity((min_temp + max_temp) / 2, dew_park$dew_point),
        month=month(date),
        year=year(date)
    ) |> 
    select(-date)

ann_mintemp_park <- ann_mintemp_park |> 
    mutate(year=year(date)) |> 
    st_drop_geometry() |> 
    rename(jan_min_temp=min_temp) |> 
    select(-date)

parks_proj <- parks_proj |> 
    left_join(ann_mintemp_park, by=c("site", "year")) |> 
    left_join(st_drop_geometry(covars_park), by=c("month", "year", "site")) # joining by site name X date is OK for the park locations
```

Make sure everything looks OK:

```{r}
ggplot(parks_proj) +
    geom_sf(data=sc_state, col="gray40") +
    geom_sf(aes(col=jan_min_temp, size=jan_min_temp), shape=1) +
    facet_wrap(~year)
```

We can double check the count data remains as expected as well. Here we plot the total monthly count across tick groups, coloring by the relative humidity (average between 2020 and 2021).

```{r, fig.width=9.5, fig.height=6}
parks_proj |> 
    unnest(counts) |> 
    group_by(month, site) |> 
    summarise(mean_rh=mean(mean_rh), tot_count=sum(count)) |> 
    ggplot() +
    geom_sf(data=sc_state, col="gray40") +
    geom_sf(aes(col=mean_rh, size=log1p(tot_count)), shape=1) +
    scale_color_viridis_c(option="magma") +
    theme_bw() +
    facet_wrap(~month, nrow=2)
```

## Combine the grid and park data and save

A separate database is also saved just for the observed points (we will use these data separately from the grid for the model comparison study).

```{r}
full_model_data <- bind_rows(
    unnest(parks_proj, counts), 
    unnest(covar_grid, counts)
)

st_write(full_model_data, "geo-files/full-model-data.shp", delete_dsn=TRUE)
st_write(unnest(parks_proj, counts), "geo-files/parks-with-covars.shp", delete_dsn=TRUE)
```





