---
title: "Model Data Preparation"
output: html_document
date: "`r Sys.Date()`"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

The output of this script is a spatial dataframe of points that is ready to be fed to the models. This will contain two "types" of points, unobserved points and observed ones. The observed points are the centroids of the park, for each time a particular site was visited, while the unobserved points lie on a 4km grid across South Carolina and will be used for prediction.

The strategy for preparing everything is first to obtain the grid using NLCD 2019 land cover classification. The land cover is one of our covariates of interest for modeling, but they also provide a grid so we can start there. Then we will build up this grid with other meteorological variables in monthly or yearly increments, depending on the variable. Separately, we then extract all the variables from the observed points, and then both spatial dataframes are combined.

The current covariates are
* Land cover (numeric label)
* Tree canopy (precent tree cover 0-100)
* Average daily temperature (Celsius)
* Cumulative daily precipitation (mm)
* Average daily relative humidity (0-100)
* Annual minimum temperature, defined as the smallest recorded minimum daily temperature throughout the year
* Annual maximum temperature, defined analogously

Load required packages
```{r, message=FALSE}
library(tidyverse)
library(sf)
library(raster, exclude=c("select"))
library(furrr)
library(lubridate)
library(nngeo)
library(ggnewscale)
```

Load the data
```{r}
parks <- read_csv("data-proc/parks-data-20-21.csv") # read data as created in tidy-parks-data.R
```

## Park data aggregation

Before extracting covariates, do some preparations of the park data itself, aggregating and dropping data where
needed. The following steps are taken preparing the data for model fitting:

- A `month` variable is added with an integer representing the Month of the visit
<!-- - The `genus` and `life_stage` variables are combined as a `group`. This will be what classifies the "species" in the Species Distribution Models -->
<!-- - Remove any "species" not specified in `groups_inc`. For now, this will be everything other than adult and nymph Amblyomma and adult Ixodes, since very few instances of the other groups exist (see below), but we can add more back in later -->
- The variables `sampling_method`, `species` and `sex` will not be used in the analysis, so we can marginalize these out to obtain counts for the factors we are interested in
- For each visitation event (i.e. `site` and `date` pair), implicit zero counts are converted to explicit zero counts for each group. Note this should also remove the issue that "zero-count" visits were entered inconsistently (some had different genus/species combinations entered but with 0, some were `NA`, etc.)


Count the number of times each group was encountered.
```{r}
parks |> 
    filter(count > 0) |> 
    count(genus, species, life_stage) |> 
    arrange(n)
```

<!-- The groups we are interested in for now are -->
<!-- ```{r} -->
<!-- groups_inc=c("Ixodes_adult", "Amblyomma_adult", "Amblyomma_nymph") -->
<!-- ``` -->


```{r}
parks_agg <- parks |> 
        select(date, site, genus:count, sampling_method, lat, long) |> 
        mutate(month=month(floor_date(date, "months"))) |> 
        # unite("group", genus, life_stage) |> 
        # filter(group %in% groups_inc) |> # A. larva too since very few collection dates
        group_by(date, site, genus, species, month, lat, long) |> # marginalize over factors not considered 
        summarise(count=sum(count)) |> 
        ungroup() |> 
        complete(nesting(date, site, month, lat, long), nesting(genus, species), fill=list(count=0))
```

Finally, we use the `lat` and `long` variables to project the data to a given CRS using the following function. `ref_raster` is a folder name containing raster files we will be using later. For now we use it only as a reference CRS, since it will very slow to convert these raster files to a different CRS.

```{r}
project_parks <- function(parks, ref_raster="mean-temp", pattern="bil$") {
    files <- list.files(paste0("geo-files/", ref_raster), pattern=pattern, full.names=TRUE)
    
    parks |>
        st_as_sf(crs="+proj=longlat +datum=WGS84", coords=c("lat", "long")) |>
        st_transform(crs(raster(files[1])))
}

parks_proj <- project_parks(parks_agg, "mean-temp") # now can get reference CRS from here
```

## Create a spatial grid using the NLCD 2019 data

These come as raster files with a 30m x 30m resolution. However, since PRISM only reports their variables at a 4km resolution, and since we will not be able to make monthly predictions at such a fine scale, the grid is aggregated to roughly 4km using the most dominant class within a 4km tile.

The rasters are then converted to points form, creating a dataframe of points along a grid. The dataframe is then filtered to only contain points within the South Carolina border.

```{r}
get_landcover_grid <- function(bounds, saved_file=NULL) {
    if (!is.null(saved_file))
        return(st_read(saved_file))
    
    lcov <- raster("geo-files/land-cover/NLCD_2019_Land_Cover_L48_20210604_yDj0p7IM7PAX2WUUipna.tiff")
    lcov_4k <- aggregate(lcov, 133, fun=modal)
    
    lcov_sp <- rasterToPoints(lcov_4k, spatial=TRUE) |> 
        st_as_sf() |> 
        rename(land_cover=NLCD_2019_Land_Cover_L48_20210604_yDj0p7IM7PAX2WUUipna) |> 
        st_transform(crs(bounds))
    
    u <- st_within(lcov_sp, bounds)
    return(lcov_sp[map_lgl(u, ~length(.x) > 0),])
}

get_canopy_grid <- function(bounds, saved_file=NULL) {
    if (!is.null(saved_file))
        return(st_read(saved_file))
    
    canopy <- raster("geo-files/land-cover/NLCD_2016_Tree_Canopy_L48_20190831_yDj0p7IM7PAX2WUUipna.tiff")
    canopy_4k <- aggregate(canopy, 133, fun=mean)
    
    canopy_sp <- rasterToPoints(canopy_4k, spatial=TRUE) |> 
        st_as_sf() |> 
        rename(tree_canopy=NLCD_2016_Tree_Canopy_L48_20190831_yDj0p7IM7PAX2WUUipna) |> 
        st_transform(crs(bounds))
    
    u <- st_within(canopy_sp, bounds)
    return(canopy_sp[map_lgl(u, ~length(.x) > 0),])
}
```

```{r}
sc_state <- st_read("geo-files/south-carolina-county-boundaries.shp") |> 
    st_transform(crs(parks_proj)) |> 
    st_union() |> 
    nngeo::st_remove_holes()

```

The grid can be loaded like this:

```
lcov_grid <- get_landcover_grid(sc_state) # grid will be proj to state (which should be proj to parks_proj)
st_write(lcov_grid, "geo-files/landcover-grid.shp", delete_dsn=TRUE)
```
However, the aggregation can take a minute or two, so instead we load the grids made previously, and combine them.

```{r}
lcov_grid <- get_landcover_grid(NULL, saved_file="geo-files/landcover-grid.shp")
canopy_grid <- get_canopy_grid(NULL, saved_file="geo-files/canopy-grid.shp") |> 
    rename(tree_canopy=tr_cnpy)

nlcd_grid <- st_join(lcov_grid, canopy_grid)
```

Now we make sure both covariates look reasonable

```{r}
ggplot(nlcd_grid) +
    geom_sf(aes(col=as.factor(land_cover)), size=1)

ggplot(nlcd_grid) +
    geom_sf(aes(col=tree_canopy), size=1) +
    scale_color_distiller(palette="Greens", direction=1)
```

Finally, we add in some blank columns that we will use for the observed park points, and convert to a `SpatialPointsDataFrame` so `raster` won't fuss later.

```{r}
nlcd_grid <- nlcd_grid |> 
    mutate(site=NA, group=list(c("Ixodes_adult", "Amblyomma_adult", "Amblyomma_nymph")), count=NA)

grid_sp <- as(nlcd_grid, "Spatial")
```

## Annual minima and maxima on grid

Now add annual temperature extrema for 2020 and 2021. Using data from [PRISM](https://prism.oregonstate.edu/recent/), search the daily minimum (maximum) temperatures for each year and find the smallest (largest) of each of these. These new values are then added to `grid_sp`, resulting in 4 new columns (these will be pivoted to only 2 columns later, so e.g. dates from 2021 don't depend on the 2020 extrema).

To do this, we define several functions that will be used for all the PRISM data extractions.

```{r}
get_raster_stack <- function(var, dates) {
    file_dates <- str_replace_all(dates, "-", "")
    pattern <- str_c(".+_(", str_c(file_dates, collapse="|"), ").+\\.bil$")
    files <- list.files(paste0("geo-files/", var), pattern=pattern, full.names=TRUE)
    as.list(stack(files))
}

load_prism_var <- function(var, dates, points) {
    stack <- get_raster_stack(var, dates)
    varname <- str_replace_all(var, "-", "_")
    
    furrr::future_map2_dfr(
        stack, dates, 
        extract_raster_points, points, 
        .options=furrr_options(seed=NULL)
    ) |> 
        rename({{varname}} := rval)
}

extract_raster_points <- function(r, date, points) {
    rvals <- raster::extract(r, points)
    
    points |> 
        st_as_sf() |> 
        mutate(rval=rvals, date=date)
}
```

First, set up a parallel session to take advantage of `furrr` and split up the expensive extraction work.

```{r}
plan(multisession, workers=4) # set up parallel session
```


```{r}
get_annual_extrema <- function(var_grid, year, min=TRUE) {
    ret <- var_grid |> 
        filter(year(date) == year) |> 
        group_by(geo=as.character(geometry))
    
    if (min) {
        ret <- slice_min(ret, min_temp, n=1, with_ties=FALSE)
        return(pull(ret, min_temp))
    }
    
    ret |> 
        slice_max(max_temp, n=1, with_ties=FALSE) |> 
        pull(max_temp)
}

# Which days to consider for finding min temp?
dates_min <- expand_grid(y=c("2020", "2021"), m=1:2, d=1:31) |> 
    transmute(date=as.Date(str_c(y, m, d, sep="-"))) |> 
    drop_na() |> # Remove Feb. 28/29-31
    pull(date)

tmin <- load_prism_var("min-temp", dates_min, grid_sp)
grid_sp$tmin_2020 <- get_annual_extrema(tmin, "2020", min=TRUE)
grid_sp$tmin_2021 <- get_annual_extrema(tmin, "2021", min=TRUE)

dates_max <- expand_grid(y=c("2020", "2021"), m=6:8, d=1:31) |> 
    transmute(date=as.Date(str_c(y, m, d, sep="-"))) |> 
    drop_na() |> # remove June 31st
    pull(date)

tmax <- load_prism_var("max-temp", dates_max, grid_sp)
grid_sp$tmax_2020 <- get_annual_extrema(tmax, "2020", min=FALSE)
grid_sp$tmax_2021 <- get_annual_extrema(tmax, "2021", min=FALSE)
```

Let's make sure everything looks reasonable:

```{r, fig.width=10, fig.height=8.5}
plot_dat <- st_as_sf(grid_sp) |> 
    pivot_longer(matches("tmin|tmax"))

ggplot() +
    geom_sf(aes(col=value), data=filter(plot_dat, name == "tmax_2020"), size=0.8) +
    geom_sf(aes(col=value), data=filter(plot_dat, name == "tmax_2021"), size=0.8) +
    scale_color_viridis_c() +
    labs(col="Temp (C)") +
    new_scale_color() +
    geom_sf(aes(col=value), data=filter(plot_dat, name == "tmin_2020"), size=0.8) +
    geom_sf(aes(col=value), data=filter(plot_dat, name == "tmin_2021"), size=0.8) +
    scale_color_viridis_c(option="magma") +
    facet_wrap(~name) +
    labs(col="Temp (C)")
```

Seems reasonable, but I'm not sure why there are those "ridges" in temp extrema though. Maybe because it's air temperature? I'm not sure if this is a common phenomenon.

Now save the full covariate grid `grid_sp` as an intermediary step.

```{r}
st_write(select(st_as_sf(grid_sp), -group), "geo-files/covar-grid-no-daily.shp", delete_dsn=TRUE)
```

## Daily meteriological variables

Again from PRISM, extract the mean temperature, precipitation, and mean relative humidity from each point of the grid. 

First, since prediction over all of SC every day is unreasonable, we will only extract the values for the first of each month. This will result in making "monthly" predictions on the grid. Note however that we will still be using the correct daily values for the park locations!

```{r}
dates_grid <- c(str_c("2020", 3:12, "01", sep="-"), str_c("2021", 3:12, "01", sep="-")) |> 
    as.Date()

# dates_grid <- dates_grid[1:(length(dates_grid)-2)]
```

```{r}
mtemp_grid <- load_prism_var("mean-temp", dates_grid, grid_sp) # No pipe cause each line expensive!
precip_grid <- load_prism_var("precipitation", dates_grid, grid_sp)
dew_grid <- load_prism_var("dew-point", dates_grid, grid_sp)
```

As an example, we can view the precipitation on April 1st, 2020:

```{r}
ggplot(filter(precip_grid, date == "2020-04-01")) +
    geom_sf(aes(col=precipitation), size=1)
```

The unobserved dataset is now almost complete. 
Just combine the three variables and convert the dew point/temperature to mean relative humidity:

```{r}
# From Alduchov & Eskridge (2002)
relative_humidity <- function(temp, dew, beta=17.625, lambda=243.04) {
    num <- exp(beta * dew / (lambda + dew))
    denom <- exp(beta * temp / (lambda + temp))
    100 * num / denom
}

covar_grid <- mtemp_grid |>
    mutate(precipitation=precip_grid$precipitation, mean_rh=relative_humidity(mean_temp, dew_grid$dew_point))
```

## Extract covariates from park locations

Now we can repeat the extraction from the centroid of each park. First, to obtain the non-daily variables, we assign a park the value of the closest point on the previously made `grid_sp`.

```{r}
grid_sp_sub <- st_as_sf(grid_sp) |> select(land_cover, tree_canopy, tmin_2020:tmax_2021) # to avoid ugly st_join behaviour
parks_proj <- st_join(parks_proj, grid_sp_sub, st_nearest_feature)
```

And now the daily covariates, again using the `load_prism_var` function.

```{r}
dates_parks <- distinct(parks_proj, date) |> pull() # only extract days when at least 1 visit occured
park_locations <- distinct(parks_proj, site, geometry)

mtemp_park <- load_prism_var("mean-temp", dates_parks, park_locations) # No pipe cause each line expensive!
precip_park <- load_prism_var("precipitation", dates_parks, park_locations)
dew_park <- load_prism_var("dew-point", dates_parks, park_locations)
```

Combine the results, and join with the parks data so that tick `count` and the covariates appear together.
```{r}
covars_park <- mtemp_park |> 
    mutate(precipitation=precip_park$precipitation, mean_rh=relative_humidity(mean_temp, dew_park$dew_point))

parks_proj <- left_join(parks_proj, st_drop_geometry(covars_park))
```

Make sure everything looks OK:

```{r}
ggplot(parks_proj) +
    geom_sf(data=sc_state, col="gray40") +
    geom_sf(aes(col=tmin_2020, size=tmin_2020), shape=1)
```

Seems to match the anomalous cold patch in the southern region during 2020 that we also saw in the grid data. 

We can double check the count data remains as expected as well. Here we plot the total monthly count across tick groups, coloring by the relative humidity (in 2021).

```{r, fig.width=9.5, fig.height=6}
parks_proj |> 
    group_by(month, site) |> 
    mutate(tot_count=sum(count)) |> 
    select(-group, -count) |> 
    distinct() |> 
    ggplot() +
    geom_sf(data=sc_state, col="gray40") +
    geom_sf(aes(col=mean_rh, size=log1p(tot_count)), shape=1) +
    scale_color_viridis_c(option="magma") +
    theme_bw() +
    facet_wrap(~month, nrow=2)
```

## Combine the grid and park data and save

We also need to merge the `temp_min` and `temp_max` 2020 and 2021 variables into two variables based on the `date` of the row. A separate database is also saved just for the observed points (we will use these data separately from the grid for the model comparison study).

```{r}
full_model_data <- bind_rows(parks_proj, unnest(covar_grid, c(group)))

full_model_data <- full_model_data |> 
    pivot_longer(c(tmin_2020, tmin_2021), "yrmin", values_to="temp_min") |> 
    filter(str_detect(yrmin, as.character(year(date)))) |> 
    pivot_longer(c(tmax_2020, tmax_2021), "yrmax", values_to="temp_max") |>
    filter(str_detect(yrmax, as.character(year(date)))) |> 
    select(-yrmin, -yrmax)

st_write(full_model_data, "geo-files/full-model-data.shp", delete_dsn=TRUE)
st_write(filter(full_model_data, !is.na(site)), "geo-files/parks-with-covars.shp", delete_dsn=TRUE)
```





